{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras notes: fun on mnist\n",
    "## MLP on mnist\n",
    "Follow the [tutorial](https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html) of tensorflow on mnist, a MLP with a single fully-connected layer and a softmax optimized in SGD with *learning rate* 0.5 is supposed to get about 92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 1s - loss: 0.3929 - acc: 0.8881     \n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 1s - loss: 0.3071 - acc: 0.9133     \n",
      " 9400/10000 [===========================>..] - ETA: 0s[0.28896924916654826, 0.91860000133514408]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, pooling, Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "def reshape(X_train, X_test):\n",
    "    # reshape [28, 28] to 784\n",
    "    (a, b, c) = X_train.shape\n",
    "    X_train = X_train.reshape((a, b*c))\n",
    "    (a, b, c) = X_test.shape\n",
    "    X_test = X_test.reshape((a, b*c))\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def scale(X_train, X_test):\n",
    "    # scale the elements to (0, 1)\n",
    "    # note: this is important to get higher accuracy, \n",
    "    # and the dataloader in tensorflow has scaled, so data is directly fed to the MLP.\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_test /= 255\n",
    "    return X_train, X_test\n",
    "\n",
    "# map the label to a binary vector with dimension 10\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "X_train, X_test = reshape(X_train, X_test)\n",
    "X_train, X_test = scale(X_train, X_test)\n",
    "\n",
    "simple_MLP = Sequential([\n",
    "    Dense(10, input_dim=784),\n",
    "    # now the model will take as input arrays of shape (*, 784)\n",
    "    # and output arrays of shape (*, 10)\n",
    "    # * represents the dimension of batch, can be different between different batches.\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "# the definition below is equivalant to the definition above\n",
    "# model = Sequential([Dense(10, input_dim=784, Activiation('softmax'))])\n",
    "sgd = SGD(lr=0.5)\n",
    "simple_MLP.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# In tutorial of tensorflow, 1000 steps with batch_size 100 are executed, which roughly are 2 epochs.\n",
    "simple_MLP.fit(X_train, y_train, batch_size=100, nb_epoch=2)\n",
    "score = simple_MLP.evaluate(X_test, y_test, batch_size=100)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more complecated MLP model, with relu activation layer on dense_layer_1 and one more fully-connected layer, we can get better accuracy (about 97%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s - loss: 0.3345 - acc: 0.8971     \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s - loss: 0.1715 - acc: 0.9487     \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s - loss: 0.1389 - acc: 0.9579     \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s - loss: 0.1209 - acc: 0.9632     \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s - loss: 0.1078 - acc: 0.9671     \n",
      "10000/10000 [==============================] - 0s     \n",
      "[0.1213740740250796, 0.96500000476837156]\n"
     ]
    }
   ],
   "source": [
    "complex_MLP = Sequential([\n",
    "    Dense(32, input_dim=784, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "sgd = SGD(lr=0.5)\n",
    "complex_MLP.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "complex_MLP.fit(X_train, y_train, batch_size=100, nb_epoch=5)\n",
    "score = complex_MLP.evaluate(X_test, y_test, batch_size=100)\n",
    "print score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network on mnist\n",
    "MLP is good, but we need more complex model to get higher accuracy, i.e CNN. Since mnist is a quite simple dataset, the difference of MLP and CNN is not evident. However, CNN performs much better on chanllenging datasets. The final accuracy of this network on mnist is about 99.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 120s - loss: 0.1488 - acc: 0.9533   \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 120s - loss: 0.0455 - acc: 0.9863   \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 121s - loss: 0.0328 - acc: 0.9896   \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 119s - loss: 0.0233 - acc: 0.9923   \n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 120s - loss: 0.0181 - acc: 0.9943   \n",
      "10000/10000 [==============================] - 6s     \n",
      "[0.026601558394356743, 0.99220000565052036]\n"
     ]
    }
   ],
   "source": [
    "# reshape the data to image-like Tensor\n",
    "def reshape_CNN(X_train, X_test):\n",
    "    # TensorFlow shape: (num_images, width, height, color_channels)\n",
    "    X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train, X_test = reshape_CNN(X_train, X_test)\n",
    "\n",
    "ml_CNN = Sequential([\n",
    "    Conv2D(32, 5, 5, border_mode='same', input_shape=(28, 28, 1), activation='relu'), #Tensorflow order: (width, height, color_channels)\n",
    "    # output shape (28, 28, 32)\n",
    "    pooling.MaxPooling2D(pool_size=(2, 2), border_mode='same'), # one maximum within 2*2 patch\n",
    "    # output shape(14, 14, 32)\n",
    "    Conv2D(64, 5, 5, border_mode='same', activation='relu'),\n",
    "    # output shape(14, 14, 64)\n",
    "    pooling.MaxPooling2D(pool_size=(2, 2), border_mode='same'),\n",
    "    # output shape(7, 7, 64)\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "ml_CNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "ml_CNN.fit(X_train, y_train, batch_size=100, nb_epoch=5)\n",
    "score = ml_CNN.evaluate(X_test, y_test, batch_size=100)\n",
    "print score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
