{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICLR-17 Papers Reading\n",
    "## [Structured Attention Networks](http://openreview.net/pdf?id=HkE0Nvqlg)\n",
    "Attention mechanism, which works as a hidden layer and provides random access to the source sequence in decoding, has been proven to be very effective on many applications. One potential problem is it fails to consider the possible *structural dependences* in the source sequence. Modeling structural dependence has also been proven to be important in many applications. Though one may argue that this *structural dependence* can be learned by the attention layer, an explicit layer is added to find the structural dependence, providing a bias.\n",
    "\n",
    "*To be continued*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Lei Yu's paper\n",
    "Given the input sequence $x = \\{x_1, ... , x_I\\}$ and target sequence $y = \\{y_1, ... ,y_J\\}$, we model on \n",
    "$$\n",
    "p(y|x) = \\prod_{j=1}^{J}p(y_j|y_{j-1}, x)\n",
    "$$\n",
    "Inspired by HMM alignment model, hidden alignment latent variables are introduced and $p(y|x)$ is computed by maginalizing out on $a$, Then:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p(y|x) &= \\sum_{a}p(y,a|x)\\\\\n",
    "&= \\sum_{j=1}^{J}p(a_j|a_{j-1},y_{j-1},x)\\dot p(y_j|a_j,y_{j-1},x)\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Problems:\n",
    "(1) Uni-version is online with better performance just than vanilla seq2seq.\n",
    "(2) Bi-version is not online with better performance than seq2seq with simple attention.\n",
    "(3) Can attention mechanism be further added to this model, for the model itself can already is a segment seq2seq model with alignment mechanism. \n",
    "\n",
    "\n",
    "Much work in NLP has focused on generative models that model joint probability distribution $p(y,x)$ over inputs and outputs. Two problems are (1) the huge dimension of the inputs; (2) complex dependencies between the inputs. \n",
    "\n",
    "The alternate is discriminative model, which models $p(y|x)$ for classification. CRF falls into this category, with the advantage that the dependencies between the in variables $x$ play no role in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
