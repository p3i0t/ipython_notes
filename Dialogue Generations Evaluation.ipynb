{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems](https://arxiv.org/pdf/1701.03079.pdf)\n",
    "This paper (from PKU) proposes a unsupervised metric for automatic evaluation of dialogue generations. It is a blended metric with both the ***similarity*** between the generation and the groundtruth and the ***ralatedness*** of the generation and the query (previous utterance). The motivations are quite natural and reasonable.\n",
    "\n",
    "## Pros\n",
    "* Unlike other referenced metrics, e.g. BLEU for machine translation, ROUGE for text summarization, which are based on exact word overlapping, a \"soft\" evaluation is proposed based on word-level similarity.\n",
    "* Unreferenced metric employs a similar matching mechanism in QA to evaluate the relatedness of the generation and the previous utterance.\n",
    "\n",
    "## Cons\n",
    "* It's not suprised that the proposed soft mechanism performs better than BLEU, but the evaluation is still limited to word-level similarity. The pooling vector could be dominated by the uncommon words, which is favorable if uncommon words in generations and groundtruth can be matched, but this doesn't even imply the generations are plausible or readable natural language sentences. This will lead to the possibility that \"poor\" generations get high scores just because several specific ***uncommon words*** have been captured by the generation. \n",
    "* Idealy, the similarity is supposed to be based on the meaning of generations, not the co-ocurrance of similar words within generations. The efficacy of this metric is not clear when repharasing happens.\n",
    "\n",
    "# [Learning to Evaluate Dialogue Responses](https://openreview.net/pdf?id=HJ5PIaseg)\n",
    "This paper is from bengio group submitted to ICLR17. The motivations of this paper and paper above are similar, both the similarity within generations and groundtruths and closedness within generations and context.\n",
    "\n",
    "## Pros\n",
    "* The model used for learning the metric is able to capture the ***semantic*** similarity, not just the word-level similarity, which is more robust.\n",
    "\n",
    "## Cons\n",
    "* The metric is learned on supervised dataset, thus is problematic. Compared to BLEU for translation evaluation, which is language-independent, this metric is not dataset-independent, and it is not clear how well this metric generalize. It's intuitively not feasible to evaluate generations of a dialogue model with this metric when they are trained on different datasets.\n",
    "* The dataset for training is not big enough. So the efficacy is not very clear.\n",
    "\n",
    "## Conclusions\n",
    "The evaluation metric of dialogue generation is still in its infancy. Several factors, like closeness, relatedness, are reasonably taken into consideration by several previous works. However, this is still far away from being enough to evaluate a \"good\" generation. The gap lies in the ability of current models to \"really\"capture the similarity and closedness we defined. Potential solutions in the future is supposed to heavily rely on the \"understanding\" of natural languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
