{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian Inference\n",
    "## What\n",
    "Bayesian inference is able to tackle many inference problems. As to the techniques for solving these problems, they can be divided into two categories: exact methods and approximate methods.\n",
    "\n",
    "$$p(\\theta|y) = \\frac{p(y|\\theta) p(\\theta)}{\\int p(y,\\theta) \\,d\\theta}$$\n",
    "\n",
    "Most of the time, analytical solutions (exact methods) are not available and neumerical integration (e.g MCMC) can be too computationally expensive.\n",
    "Variational inference is an approximate method which is thought to be more efficient than MCMC. The word \"variational\" comes from the variational calculusï¼Œwhich means it works on functionals. \n",
    "\n",
    "In order to approximate a true distribution $P(x)$, we have a set of distribution candidates $\\{Q_i(x; \\theta)\\}$ and wish to select the one that minimize the KL divergence:\n",
    "$$D_{KL}(Q(x)||P(x|D)) = \\int Q(x) \\ln \\frac{Q(x)}{P(x|D)}\\,dx $$\n",
    "where $D$ is the data set and x are unobserved variables.\n",
    "\n",
    "## Transformation\n",
    "Then we rewrite the KL formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "D_{KL}(Q(x)||P(x|D)) &= \\int Q(x) \\ln \\frac{Q(x)}{P(x|D)}\\,dx\\\\  \n",
    "&= -\\int Q(x) \\log \\frac{P(x|D)}{Q(x)}\\,dx\\\\\n",
    "&= -\\int Q(x) \\big[\\log \\frac{P(x,D)}{Q(x)} - \\log P(D) \\big]\\,dx\\\\\n",
    "&= -\\int Q(x) \\log \\frac{P(x,D)}{Q(x)} \\,dx + \\log P(D)\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Next, replace the conditional $P(x|D)$ with a joint $P(x, D)$ and a prior P(D). The reason for making this rewrite is that for Bayesian networks with exponential family nodes, the $log P(x, D)$ term will be a be a very simple sum of node energy terms, whereas $log P(x|D)$ is more complicated. This will simplify later computations.\n",
    "\n",
    "Then we have:\n",
    "$$ \n",
    "\\log P(D) = D_{KL}(Q(x)||P(x|D)) + L\n",
    "$$\n",
    "where \n",
    "$$ \n",
    "L = \\int Q(x) \\log \\frac{P(x,D)}{Q(x)} \\,dx \n",
    "$$ \n",
    "is called Evidence Lower BOund (ELBO).\n",
    "\n",
    "Bacause $\\log P(D)$ can be seen as a constant, minimizing $D_{KL}(Q(x)||P(x|D))$ is to maximizing ELBO $L$.\n",
    "\n",
    "The approximation is to minimising the *variational free energy F*, which can be intepreted as a \n",
    "\n",
    "## How\n",
    "Variational inference is not satisfactory because: (1) the cadidates used to approximate are limited to exponential distribution family; (2)the mean-field assumption is too strong that components of variable $x$ (vector) are independent.\n",
    "\n",
    "In variational autoencoder, neural network is employed to approximate due to its ability to fit data.\n",
    "\n",
    "## MCMC estimator\n",
    "\n",
    "A MCMC estimator of gradient of and expectation w.r.t a general function.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\nabla_{\\phi} E_{q_{\\phi}(z)}\\big[f(z)\\big] &= \\nabla_{\\phi} \\int q_{\\phi}(z)f(z) \\,dz \\\\\n",
    "&= \\int \\big[\\nabla_{\\phi}q_{\\phi}(z)\\big] f(z) \\,dz \\\\\n",
    "&= \\int \\big[\\nabla_{\\phi}q_{\\phi}(z)\\big] \\frac{q_{\\phi}(z)}{q_{\\phi}(z)}f(z) \\,dz \\\\\n",
    "&= \\int \\big[\\nabla_{\\phi}\\log q_{\\phi}(z))\\big] q_{\\phi}(z)f(z) \\,dz \\\\\n",
    "&= E_{q_{\\phi}(z)}\\big[f(z) \\nabla_{\\phi}\\log q_{\\phi}(z))\\big]\\\\\n",
    "&\\simeq \\frac{1}{L}\\sum_{l=1}^{L} f(z)\\nabla_{\\phi} \\log q_{\\phi}(z^{(l)}) \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
