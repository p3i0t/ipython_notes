{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes: Support Vector Machine\n",
    "## Margin and Support Vectors\n",
    "Given a training set $D = {(x_i, y_i), ..., (x_n, y_n)}, y_i \\in {-1, +1}$, where $x$ usually is a feature vector to represent a sample and $y$ is the label of the sample, a classifier, trained on the training set $D$, learns to discriminate the samples by their labels. Technichally, the goal is to find a hyperplane to seperate the samples. The hyperplane can be written into:\n",
    "$$\n",
    "    w^{T}x + b = 0,\n",
    "$$ where $w = (w_1, w_2, ... , w_d) \\in R^{d}$ is the normal vector and $b$ is the bias.\n",
    "\n",
    "Actually, there can be a lot of hyperplane candidates and we should choose the $best$ one of them. The problem is how do we define the \"best\" here.\n",
    "\n",
    "First, we define what are support vectors. Of all the samples that reside on different sides of a hyperplane, some are closer to the hyperplane, while the others are farther. The vectors whose distances to the hyperplane are smallest are called support vectors. Obviously, support vectors are spreaded on both sides of the hyperplane.\n",
    "\n",
    "Given any $x$, the distance between $x$ and the hyperplane is:\n",
    "$$\n",
    "r = \\frac{|w^{T}x + b|}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "A good hyperplace is supposed to seperate all the samples to different sides, so the best hyperplane is the one right in the middle. We denote $r^{-}$ to be the smallest distance between positive ($y=+1$) samples and hyperplane and $r^{+}$ to be the smallest distance between negtive ($y=-1$) samples and hyperplane. We have $r^{-} = r^{+} = r'$. We can assume the two parallel lines that all support vectors lie to be:\n",
    "\\begin{cases}\n",
    "&w^{T}x_i + b = b' ,\\text{ if } y_i= +1,\\\\ \n",
    "&w^{T}x_i + b = -b', \\text{ if } y_i= -1\n",
    "\\end{cases}\n",
    "\n",
    "Assume the hyperplance can classify all the samples correctly, we have:\n",
    "\\begin{cases}\n",
    "&\\frac{|w^{T}x_i +b|}{\\|w\\|} \\geq r^{+} ,\\text{ if } y_i= +1,\\\\ \n",
    "&\\frac{|w^{T}x_i +b|}{\\|w\\|} \\geq r^{-}, \\text{ if } y_i= -1\n",
    "\\end{cases}\n",
    "\n",
    "After scalling with $r'$, we have:\n",
    "\\begin{cases}\n",
    "&w^{T} x_i + b \\geq +1 ,\\text{ if } y_i= +1,\\\\ \n",
    "&w^{T} x_i + b \\leq -1, \\text{ if } y_i= -1\n",
    "\\end{cases}\n",
    "\n",
    "Then the sum of distances between the hyperplace and positive and negtive support vectors $\\gamma = \\frac{2}{\\|w\\|}$ is called $margin$. \n",
    "## Primal form\n",
    "Our goal is to maximize this margin. \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{maxmize}}\n",
    "& & \\frac{2}{\\|w\\|} \\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^{T} x_i + b) \\geq +1, \\; i = 1, \\ldots, n.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "It can be rewritten to:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{minimize}}\n",
    "& & \\frac{1}{2}\\|w\\|^{2} \\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^{T} x_i + b) \\geq +1, \\; i = 1, \\ldots, n.\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "which is the primal form of SVM optimization problem.\n",
    "\n",
    "## Dual form \n",
    "The primal form above can be transformed into a dual form by introducing lagrangian variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
